{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Read the CSV file\n",
    "data = pd.read_csv('train.csv')  # Adjust the file path as needed\n",
    "\n",
    "# Step 2: Shuffle the data\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "train_size = 0.8  # 80% of the data for training\n",
    "test_size = 1 - train_size\n",
    "\n",
    "# Step 4: Split the data into training and testing sets\n",
    "train_data, test_data = train_test_split(data, test_size=test_size, random_state=42)\n",
    "\n",
    "# Step 5: Save the split datasets into separate CSV files if needed\n",
    "train_data.to_csv('train_data.csv', index=False)\n",
    "test_data.to_csv('test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Кастомный датасет, который загружает csv и на каждое обращение возвращает картинку и 28 точек с ней ассоциированных\n",
    "import math\n",
    "\n",
    "from PIL import ImageDraw\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.transforms import v2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file,transform = True,device=\"cpu\"):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.target_size = (224,224)\n",
    "        self.device = device\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.data.iloc[idx, 0]  # Assuming the filename column is the first one\n",
    "        image = Image.open(os.path.join(\"train\", img_name))\n",
    "        if image.mode == 'L':\n",
    "                image = image.convert('RGB')\n",
    "        key_points = self.data.iloc[idx, 1:].values.astype('float32')# Assuming key points start from second column\n",
    "        if self.transform:\n",
    "            image_tensor,key_points = self._transform(image, key_points)\n",
    "        else:\n",
    "            original_image_size = image.size[::-1]  # (width, height)\n",
    "            target_image_size = (224, 224)  # (width, height)\n",
    "            scale_x = target_image_size[0] / original_image_size[0]\n",
    "            scale_y = target_image_size[1] / original_image_size[1]\n",
    "            # key_points = self.data.iloc[idx, 1:].values.astype('float32')\n",
    "            resized_key_points = key_points.reshape(-1, 2) * [scale_x, scale_y]\n",
    "            key_points = resized_key_points.flatten()  # Flatten back to 1D array\n",
    "            key_points = torch.tensor(key_points.reshape(-1)).float()\n",
    "            image = v2.Resize((224,224))(image)\n",
    "            image_tensor = v2.ToTensor()(image)\n",
    "            # image_tensor = v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(image_tensor)\n",
    "            \n",
    "        # self._visualize_keypoints(image_tensor, key_points)\n",
    "        return image_tensor, key_points\n",
    "    \n",
    "    def _transform(self, image, key_points):\n",
    "        original_image_size = image.size[::-1]  # (width, height)\n",
    "        target_image_size = (224, 224)  # (width, height)\n",
    "        scale_x = target_image_size[0] / original_image_size[0]\n",
    "        scale_y = target_image_size[1] / original_image_size[1]\n",
    "        # key_points = self.data.iloc[idx, 1:].values.astype('float32')\n",
    "        resized_key_points = key_points.reshape(-1, 2) * [scale_x, scale_y]\n",
    "        key_points = resized_key_points.flatten()\n",
    "        image = v2.Resize((224,224))(image)\n",
    "        \n",
    "        if np.random.rand() < 0.5:\n",
    "            \n",
    "            image = v2.functional.hflip(image)\n",
    "            num_keypoints = key_points.size // 2\n",
    "            image_width = self.target_size[0]\n",
    "            center_x = image_width / 2\n",
    "            \n",
    "            # Adjust the x-coordinates of keypoints based on their semantic positions\n",
    "            for i in range(num_keypoints):\n",
    "                x = key_points[i * 2]\n",
    "                if x < center_x:  # Only flip keypoints on the left side of the image\n",
    "                    distance_to_center = center_x - x\n",
    "                    key_points[i * 2] = center_x + distance_to_center\n",
    "                else:\n",
    "                    distance_to_center = x - center_x\n",
    "                    key_points[i * 2] = center_x - distance_to_center\n",
    "                    \n",
    "            pairs_to_swap = [(0, 3), (1, 2), (4, 9), (5, 8), (6, 7), (11, 13)]\n",
    "            for a, b in pairs_to_swap:\n",
    "                key_points[a*2], key_points[b*2] = key_points[b*2], key_points[a*2]\n",
    "                key_points[a*2+1], key_points[b*2+1] = key_points[b*2+1], key_points[a*2+1]\n",
    "        \n",
    "        angle = np.random.uniform(-180, 180)  # Rotate between -30 and 30 degrees\n",
    "        image = v2.functional.rotate(image,-angle)\n",
    "        theta = np.radians(angle)\n",
    "        rotation_matrix = np.array([\n",
    "            [math.cos(theta), -math.sin(theta)],\n",
    "            [math.sin(theta), math.cos(theta)]\n",
    "        ])\n",
    "        key_points_xy = key_points.reshape(-1, 2)\n",
    "        rotated_key_points_xy = np.dot(rotation_matrix, (key_points_xy - [112, 112]).T).T + [112, 112]\n",
    "        rotated_key_points = rotated_key_points_xy.reshape(-1)\n",
    "        key_points = rotated_key_points_xy.reshape(-1)\n",
    "        \n",
    "        image = v2.ToTensor()(image)\n",
    "        # image = v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(image)\n",
    "\n",
    "        # Reshape key_points back to a one-dimensional array\n",
    "        key_points = torch.tensor(key_points.reshape(-1)).float()\n",
    "        \n",
    "        return image, key_points\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _visualize_keypoints(self, image, key_points):\n",
    "        # Convert tensor to PIL image for visualization\n",
    "        image = v2.ToPILImage()(image)\n",
    "        draw = ImageDraw.Draw(image)\n",
    "\n",
    "        # Convert keypoints to numpy array and reshape to (num_points, 2)\n",
    "        key_points = key_points.numpy().reshape(-1, 2)\n",
    "        \n",
    "        # Draw keypoints on the image\n",
    "        for (x, y) in key_points:\n",
    "            # Draw a circle for each keypoint\n",
    "            draw.ellipse((x-2, y-2, x+2, y+2), fill='red')\n",
    "        \n",
    "        # Display the image with keypoints\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используем GPU, если есть\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional neural network которая с нормализацией в виде BatchNorm и регуляризацией Dropout\n",
    "class FacePointsClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FacePointsClassification, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),\n",
    "            \n",
    "            nn.BatchNorm2d(32),  # Batch normalization\n",
    "            nn.Dropout(0.2),  # Dropout regularization\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),  # Batch normalization\n",
    "            # nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),  # Batch normalization\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.BatchNorm2d(256),  # Batch normalization\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.BatchNorm2d(512),  # Batch normalization\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 512),  # Increase model capacity\n",
    "            nn.Dropout(0.5),  # Dropout regularization\n",
    "            nn.Linear(512, 28)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "class HourglassBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(HourglassBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv4 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.bn1(self.conv1(x)))\n",
    "        x = nn.functional.relu(self.bn2(self.conv2(x)))\n",
    "        skip = x\n",
    "        x = self.downsample(x)\n",
    "        x = nn.functional.relu(self.bn3(self.conv3(x)))\n",
    "        x = nn.functional.relu(self.bn4(self.conv4(x)))\n",
    "        x = nn.functional.interpolate(x, scale_factor=2, mode='nearest')\n",
    "        x += skip\n",
    "        return x\n",
    "\n",
    "class HourglassNetwork(nn.Module):\n",
    "    def __init__(self, num_stages=2, num_channels=32, num_keypoints=28, dropout=0.5, weight_decay=1e-5):\n",
    "        super(HourglassNetwork, self).__init__()\n",
    "        self.num_stages = num_stages\n",
    "        self.num_channels = num_channels\n",
    "        self.init_conv = nn.Conv2d(3, num_channels, kernel_size=7, stride=2, padding=3)\n",
    "        \n",
    "        # Hourglass blocks\n",
    "        self.hourglass_stages = nn.ModuleList([\n",
    "            self._build_hourglass_stage(num_channels) for _ in range(num_stages)\n",
    "        ])\n",
    "        \n",
    "        # Keypoints head\n",
    "        self.keypoints_head = nn.Conv2d(num_channels, num_keypoints, kernel_size=1)\n",
    "\n",
    "        # Regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def _build_hourglass_stage(self, num_channels):\n",
    "        layers = []\n",
    "        for _ in range(3):\n",
    "            layers.append(HourglassBlock(num_channels, num_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial convolution\n",
    "        x = nn.functional.relu(self.init_conv(x))\n",
    "        \n",
    "        intermediate_outputs = []\n",
    "        # Hourglass blocks\n",
    "        for stage in self.hourglass_stages:\n",
    "            x = stage(x)\n",
    "            intermediate_outputs.append(x)\n",
    "        \n",
    "        # Keypoints head\n",
    "        keypoints = self.keypoints_head(x)\n",
    "        return keypoints, intermediate_outputs\n",
    "\n",
    "# Additional layers for post-processing\n",
    "class PostProcessingModule(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(PostProcessingModule, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Combined network\n",
    "class CombinedNetwork(nn.Module):\n",
    "    def __init__(self, num_stages=1, num_channels=64, num_keypoints=28, postprocessing_input_size=351232, postprocessing_output_size=28):\n",
    "        super(CombinedNetwork, self).__init__()\n",
    "        self.hourglass = HourglassNetwork(num_stages, num_channels, num_keypoints)\n",
    "        self.postprocessing = PostProcessingModule(postprocessing_input_size, postprocessing_output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keypoints, intermediate_outputs = self.hourglass(x)\n",
    "        processed_output = self.postprocessing(keypoints)\n",
    "        return processed_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=28):  # 28 coordinates (14 keypoints * 2)\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.inc = DoubleConv(in_channels, 64)\n",
    "        self.down1 = nn.MaxPool2d(2)\n",
    "        self.conv1 = DoubleConv(64, 128)\n",
    "        \n",
    "        self.down2 = nn.MaxPool2d(2)\n",
    "        self.conv2 = DoubleConv(128, 256)\n",
    "        \n",
    "        self.down3 = nn.MaxPool2d(2)\n",
    "        self.conv3 = DoubleConv(256, 512)\n",
    "        \n",
    "        self.up1 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.conv4 = DoubleConv(512, 256)\n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.conv5 = DoubleConv(256, 128)\n",
    "        \n",
    "        self.up3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.conv6 = DoubleConv(128, 64)\n",
    "        \n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 224 * 224, 16),  # Fully connected layer with 1024 units\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(16, out_channels)  # Output layer with 28 units (14 keypoints * 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.conv1(self.down1(x1))\n",
    "        x3 = self.conv2(self.down2(x2))\n",
    "        x4 = self.conv3(self.down3(x3))\n",
    "        \n",
    "        x = self.up1(x4)\n",
    "        x = self.crop_and_concat(x, x3)\n",
    "        x = self.conv4(x)\n",
    "        \n",
    "        x = self.up2(x)\n",
    "        x = self.crop_and_concat(x, x2)\n",
    "        x = self.conv5(x)\n",
    "        \n",
    "        x = self.up3(x)\n",
    "        x = self.crop_and_concat(x, x1)\n",
    "        x = self.conv6(x)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def crop_and_concat(self, upsampled, bypass):\n",
    "        \"\"\"\n",
    "        Crop the `bypass` tensor to match the size of the `upsampled` tensor and concatenate them.\n",
    "        \"\"\"\n",
    "        _, _, h, w = upsampled.size()\n",
    "        bypass = self.center_crop(bypass, h, w)\n",
    "        return torch.cat((upsampled, bypass), 1)\n",
    "    \n",
    "    def center_crop(self, layer, target_height, target_width):\n",
    "        _, _, layer_height, layer_width = layer.size()\n",
    "        diff_y = (layer_height - target_height) // 2\n",
    "        diff_x = (layer_width - target_width) // 2\n",
    "        return layer[:, :, diff_y:(diff_y + target_height), diff_x:(diff_x + target_width)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU)\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(SingleConv, self).__init__()\n",
    "        self.single_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.single_conv(x)\n",
    "\n",
    "class MediumUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=28):  # 28 coordinates (14 keypoints * 2)\n",
    "        super(MediumUNet, self).__init__()\n",
    "\n",
    "        self.inc = SingleConv(in_channels, 64) \n",
    "        self.down1 = nn.Sequential(nn.MaxPool2d(2), SingleConv(64, 128)) \n",
    "        self.down2 = nn.Sequential(nn.MaxPool2d(2), SingleConv(128, 256)) \n",
    "        self.down3 = nn.Sequential(nn.MaxPool2d(2), SingleConv(256, 512))\n",
    "        self.down4 = nn.Sequential(nn.MaxPool2d(2), SingleConv(512, 1024))\n",
    "        \n",
    "        self.up1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.conv1 = SingleConv(1024, 512)\n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.conv2 = SingleConv(512, 256)\n",
    "        \n",
    "        self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.conv3 = SingleConv(256, 128)\n",
    "        \n",
    "        self.up4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.conv4 = SingleConv(128, 64)\n",
    "\n",
    "        # Global Average Pooling instead of flattening\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5)\n",
    "        x = torch.cat([x, x4], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.up2(x)\n",
    "        x = torch.cat([x, x3], dim=1)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        x = self.up3(x)\n",
    "        x = torch.cat([x, x2], dim=1)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        x = self.up4(x)\n",
    "        x = torch.cat([x, x1], dim=1)\n",
    "        x = self.conv4(x)\n",
    "        \n",
    "        x = self.gap(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем предобученную модель ResNet и устанавливаем ей Feature Layer так, как нужно нам -- 14 точек на лице\n",
    "# model = models.resnet18()\n",
    "# model.train()\n",
    "# num_features = model.fc.in_features\n",
    "# num_keypoints = 14  \n",
    "# model.fc = nn.Sequential(\n",
    "#     nn.Dropout(0.2),\n",
    "#     nn.Linear(num_features, 1024),\n",
    "#     nn.ReLU(inplace=True),\n",
    "#     nn.Linear(1024, num_keypoints * 2)  # Каждая точка -- это два числа: x и y\n",
    "# )\n",
    "# model.to(device)\n",
    "# if os.path.exists('resnet.pth'):\n",
    "#     model.load_state_dict(torch.load('resnet.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используем свою модель на основе Hourglass Network и загружаем на cuda\n",
    "# model = CombinedNetwork().to(device)\n",
    "# model.load_state_dict(torch.load('model.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MediumUNet().to(device)\n",
    "model.load_state_dict(torch.load('model_7.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = FacePointsClassification().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(csv_file='train_data.csv', device=device, transform=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataset = CustomDataset(csv_file='test_data.csv', device=device, transform=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "# Определяем функцию потерь по заданию и алгоритм оптимизации\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0074, weight_decay=1e-4)\n",
    "lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]d:\\miniconda3\\envs\\mlenv312\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 500/500 [04:30<00:00,  1.85it/s]\n",
      "100%|██████████| 1000/1000 [00:29<00:00, 33.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.007392698895184605]\n",
      "Epoch 1/100, Train Loss: 214.03321531677247. Val loss: 321.8982024130821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [04:21<00:00,  1.91it/s]\n",
      "100%|██████████| 1000/1000 [00:29<00:00, 34.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.007370824394863569]\n",
      "Epoch 2/100, Train Loss: 203.2440328979492. Val loss: 164.92266852378845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [04:20<00:00,  1.92it/s]\n",
      "100%|██████████| 1000/1000 [00:29<00:00, 34.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0073344628276961485]\n",
      "Epoch 3/100, Train Loss: 194.62450736236573. Val loss: 262.2246756086349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [04:17<00:00,  1.94it/s]\n",
      "100%|██████████| 1000/1000 [00:26<00:00, 38.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.007283757696175935]\n",
      "Epoch 4/100, Train Loss: 185.66132890319824. Val loss: 387.2910668554306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:54<00:00,  2.13it/s]\n",
      "100%|██████████| 1000/1000 [00:26<00:00, 38.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.007218909110292068]\n",
      "Epoch 5/100, Train Loss: 168.24881130981444. Val loss: 272.7255507297516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:55<00:00,  2.13it/s]\n",
      "100%|██████████| 1000/1000 [00:26<00:00, 38.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00714017299778653]\n",
      "Epoch 6/100, Train Loss: 186.17786496734618. Val loss: 325.5882059555054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [04:19<00:00,  1.93it/s]\n",
      "100%|██████████| 1000/1000 [00:28<00:00, 34.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.007047860094124273]\n",
      "Epoch 7/100, Train Loss: 176.698814994812. Val loss: 119.75628626632691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [04:18<00:00,  1.93it/s]\n",
      "100%|██████████| 1000/1000 [00:29<00:00, 34.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.006942334716162295]\n",
      "Epoch 8/100, Train Loss: 179.59666694641112. Val loss: 167.16221603870392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [04:20<00:00,  1.92it/s]\n",
      "100%|██████████| 1000/1000 [00:28<00:00, 34.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.006824013324357456]\n",
      "Epoch 9/100, Train Loss: 157.91110636138916. Val loss: 259.414092751503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [04:19<00:00,  1.93it/s]\n",
      "100%|██████████| 1000/1000 [00:29<00:00, 34.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.006693362879187306]\n",
      "Epoch 10/100, Train Loss: 151.6955668563843. Val loss: 345.08406630420683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [04:17<00:00,  1.94it/s]\n",
      "100%|██████████| 1000/1000 [00:29<00:00, 34.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.006550898998270421]\n",
      "Epoch 11/100, Train Loss: 168.68543017578125. Val loss: 160.29490923166276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [04:17<00:00,  1.94it/s]\n",
      "100%|██████████| 1000/1000 [00:29<00:00, 34.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.006397183921459223]\n",
      "Epoch 12/100, Train Loss: 147.59846968841552. Val loss: 101.26050327205658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 156/500 [01:17<02:50,  2.01it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     13\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 15\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     17\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Validate the model on the validation set\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, key_points in tqdm(train_dataloader):\n",
    "        images = images.to(device)\n",
    "        key_points = key_points.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, key_points)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataloader.dataset)\n",
    "    \n",
    "    # Validate the model on the validation set\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for val_images, val_key_points in tqdm(val_dataloader):\n",
    "            val_images = val_images.to(device)\n",
    "            val_key_points = val_key_points.to(device)\n",
    "            val_outputs = model(val_images)\n",
    "            val_loss += criterion(val_outputs, val_key_points).item() * val_images.size(0)\n",
    "    \n",
    "    val_loss /= len(val_dataloader.dataset)\n",
    "\n",
    "    # Step the scheduler with the validation loss\n",
    "    lr_scheduler.step()\n",
    "    print(lr_scheduler.get_last_lr())\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss}. Val loss: {val_loss}')\n",
    "    torch.save(model.state_dict(), 'model_8.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\mlenv312\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 1000/1000 [00:19<00:00, 50.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100, Loss: 13472.1572265625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for images, key_points in tqdm(test_dataloader):\n",
    "        images, key_points = images.to(device), key_points.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, key_points)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.ImageDraw\n",
    "\n",
    "# model = CombinedNetwork().to(device)\n",
    "# model.load_state_dict(torch.load('model.pth', map_location=device))\n",
    "model.eval()\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "    ]\n",
    ")\n",
    "folder_path = \"test\"\n",
    "results = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # Adjust file extensions as needed\n",
    "            \n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            image = Image.open(image_path)\n",
    "            original_image_size = image.size[::-1]\n",
    "            target_image_size = (224,224)\n",
    "            scale_x = original_image_size[0] / target_image_size[0]\n",
    "            scale_y = original_image_size[1] / target_image_size[1]\n",
    "            \n",
    "            image = v2.Resize((224,224))(image)\n",
    "            image_tensor = v2.ToTensor()(image)\n",
    "            back = transforms.Compose([transforms.ToPILImage(), transforms.Resize(original_image_size)])\n",
    "            image_tensor = v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(image_tensor)\n",
    "            outputs = model(image_tensor.unsqueeze(0).to(device))\n",
    "            outputs[::2] *= scale_x\n",
    "            outputs[1::2] *= scale_y\n",
    "            image = back(image_tensor.squeeze(0))\n",
    "            # draw = PIL.ImageDraw.Draw(image)\n",
    "            outputs = outputs.squeeze().cpu().numpy().tolist()\n",
    "            outputs = [round(coord) for coord in outputs]\n",
    "            # for i in range(outputs.shape[0] // 2):\n",
    "                # draw.ellipse((outputs[2*i] - 3, outputs[2*i+1] - 3,outputs[2*i] + 3, outputs[2*i+1] + 3), 'red',2)\n",
    "                \n",
    "            results.append([filename] + outputs)\n",
    "columns = ['filename']\n",
    "for i in range(1, 15):\n",
    "    columns.extend([f'x{i}', f'y{i}'])\n",
    "df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "df.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('train.csv')\n",
    "idx = 0\n",
    "img_name = test.iloc[idx, 0]  # Assuming the filename column is the first one\n",
    "image = Image.open(os.path.join(\"train\", img_name))\n",
    "if image.mode == 'RGB':  # Grayscale image\n",
    "    # Convert grayscale image to RGB\n",
    "    image = image.convert('L')\n",
    "image = image.convert('RGB')\n",
    "key_points = test.iloc[idx, 1:].values.astype('float32')# Assuming key points start from second column\n",
    "for key in key_points:\n",
    "    key = torch.tensor(key).to(device)\n",
    "colors = [\n",
    "                'red', 'green', 'blue', 'yellow', 'purple', 'orange', 'cyan', 'magenta', 'lime', 'pink',\n",
    "                'teal', 'lavender', 'brown', 'beige'\n",
    "            ]\n",
    "\n",
    "# image = back(image_tensor)\n",
    "draw = PIL.ImageDraw.Draw(image)\n",
    "for i in range(int(key_points.shape[0] / 2)):\n",
    "    draw.ellipse((key_points[2*i] - 3, key_points[2*i+1] - 3,key_points[2*i] + 3, key_points[2*i+1] + 3), colors[i],2)\n",
    "image.show()\n",
    "image.save(\"train.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00008.jpg\n",
      "[ 51.190086  44.68474   94.28988   47.75281  139.37932   50.42398\n",
      " 182.89108   55.827522  59.352192  64.41544   75.81185   64.83055\n",
      "  90.545876  67.84083  140.71674   71.36037  157.02075   70.74526\n",
      " 173.0281    73.009544 109.552185 116.9162    77.74251  152.8554\n",
      " 108.36851  156.51566  139.79965  155.88333 ]\n",
      "00014.jpg\n",
      "[ 31.290686  76.16387   70.600845  61.86674  111.14813   47.055336\n",
      " 151.23613   34.79238   46.60512   89.56136   61.495636  83.385864\n",
      "  75.87622   80.578125 120.45459   65.2323   134.43771   57.639114\n",
      " 149.34256   53.929615 110.225494 115.002426  94.781265 157.88368\n",
      " 123.26252  149.50348  150.8914   136.93298 ]\n",
      "00015.jpg\n",
      "[ 52.039543  42.14046   97.6141    46.126583 143.35995   50.624264\n",
      " 189.0402    59.28097   60.321712  61.948578  77.5386    62.60893\n",
      "  93.15272   66.83176  143.97093   72.493256 160.96423   72.21099\n",
      " 177.54556   75.63316  110.73839  116.52202   75.574135 151.8795\n",
      " 107.40419  157.14986  139.5967   157.92072 ]\n",
      "00017.jpg\n",
      "[ 32.202423  71.61119   72.92153   59.198013 114.75289   46.33605\n",
      " 156.19565   36.31402   47.014626  85.99451   62.41504   80.489105\n",
      "  77.15235   78.46751  123.1734    65.217026 137.75368   58.34658\n",
      " 153.14688   55.399292 110.232315 115.182495  92.33251  157.85329\n",
      " 121.66933  150.89182  150.32478  139.6467  ]\n",
      "00023.jpg\n",
      "[ 34.64709   64.622505  75.54011   55.74061  119.22183   45.49425\n",
      " 161.51288   37.657356  47.96454   81.272194  63.63663   77.174515\n",
      "  77.99211   75.68444  126.16842   64.796    141.46498   59.397316\n",
      " 157.34435   57.07205  109.57131  116.090805  89.77832  159.01878\n",
      " 119.62335  153.52348  149.30754  144.00131 ]\n",
      "00029.jpg\n",
      "[202.00026   87.50676  180.54787  125.46319  155.59158  164.28424\n",
      " 132.90756  202.16974  179.53542   83.48613  171.45853   97.66829\n",
      " 161.15773  110.63869  135.27017  154.38086  128.6288   168.39343\n",
      " 119.4454   182.7276   107.10303  109.45059   90.995544  63.625\n",
      "  73.09467   89.422646  58.77041  116.21124 ]\n",
      "00031.jpg\n",
      "[ 27.36384  101.7668    58.56084   77.81263   91.930046  51.907497\n",
      " 125.05037   27.461567  45.451023 110.33101   57.260487 100.50064\n",
      "  69.16939   93.295204 106.035736  65.69967  117.00051   54.591125\n",
      " 129.37732   46.508812 111.04289  115.43121  109.7633   159.58226\n",
      " 133.62505  143.28433  155.40018  123.203476]\n",
      "00033.jpg\n",
      "[ 85.43497   26.88619  123.49756   46.755657 162.2713    67.56173\n",
      " 199.19241   90.71897   84.30415   47.399323  98.916115  54.231102\n",
      " 110.99413   63.57865  153.95618   87.116295 168.80452   93.14287\n",
      " 182.39523  101.73289  107.96035  115.36279   64.68804  134.53098\n",
      "  90.815094 150.6576   118.77099  162.58119 ]\n",
      "00037.jpg\n",
      "[ 28.618164  75.16516   69.12261   60.442318 111.34243   44.892567\n",
      " 153.18576   31.981503  44.409386  89.43279   59.761887  83.07332\n",
      "  74.428246  80.01023  120.77095   63.537807 135.4226    55.769337\n",
      " 151.11374   51.828857 110.705635 115.66307   95.13298  160.50214\n",
      " 124.72204  151.51901  153.35135  138.30563 ]\n",
      "00044.jpg\n",
      "[ 58.54743   37.16913  103.839615  44.805553 148.56436   53.500027\n",
      " 193.59323   66.77034   64.96913   57.293926  82.08544   59.255466\n",
      "  97.32527   65.17258  146.93071   75.4359   163.92198   76.57543\n",
      " 180.21361   81.620674 110.1198   115.4258    71.66377  147.43765\n",
      " 103.009705 155.61888  134.72821  159.30264 ]\n",
      "00050.jpg\n",
      "[ 27.82752   88.93796   63.339294  69.27234  101.043175  48.355244\n",
      " 138.2746    29.131653  44.709984 100.27891   58.205875  92.08969\n",
      "  71.4644    86.7517   112.996635  64.48668  125.67182   54.950783\n",
      " 139.61208   48.745945 110.691666 115.5211   102.61808  160.18826\n",
      " 129.16046  147.3155   154.30154  130.43483 ]\n",
      "00063.jpg\n",
      "[ 42.737373  52.545906  87.238655  49.94269  132.20218   47.656433\n",
      " 176.92502   49.04881   53.88206   70.30456   70.696266  68.47418\n",
      "  86.35795   70.30511  136.09705   68.69378  152.36838   65.78362\n",
      " 168.69066   66.69059  110.308655 115.91625   81.31695  154.43779\n",
      " 112.76756  155.06915  144.18298  151.12604 ]\n",
      "00065.jpg\n",
      "[ 36.3714    57.380817  81.171974  51.18137  126.80376   44.875275\n",
      " 172.35287   42.20614   49.28672   74.79742   66.195885  71.57178\n",
      "  81.99158   72.02842  132.30475   65.78764  148.72177   61.39943\n",
      " 165.5383    60.944126 110.57703  116.03744   84.745224 157.66609\n",
      " 116.70196  155.44023  148.34587  148.64073 ]\n",
      "00069.jpg\n",
      "[ 43.651196  50.711193  89.07792   48.88274  134.6856    47.662197\n",
      " 180.53989   50.5131    54.489716  68.94861   71.5672    67.37961\n",
      "  87.47191   69.7221   137.8592    69.04262  154.56964   66.38534\n",
      " 171.34851   67.87595  111.224075 116.180466  80.47988  154.68031\n",
      " 112.677765 155.96535  144.51608  152.60281 ]\n",
      "00079.jpg\n",
      "[ 51.81038   41.27858   97.66063   45.299564 144.018     49.953934\n",
      " 190.28647   58.544094  59.912872  61.47351   77.20615   62.16931\n",
      "  92.89097   66.37328  144.31966   71.95393  161.51501   71.663864\n",
      " 178.46909   75.12359  111.229095 116.7948    75.504074 152.65405\n",
      " 107.85019  157.79784  140.27113  158.39696 ]\n",
      "00080.jpg\n",
      "[ 73.95118   29.10255  115.82654   44.4165   157.52571   60.945248\n",
      " 198.70276   81.08036   75.8042    49.80065   91.762405  54.738205\n",
      " 105.32028   63.068615 151.62715   81.914276 167.75351   86.0663\n",
      " 182.72571   93.55256  108.498344 114.80849   65.9302   139.32938\n",
      "  94.83945  152.70854  124.75603  161.65997 ]\n",
      "00084.jpg\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m     draw\u001b[38;5;241m.\u001b[39mellipse((output[i] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m, output[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m, output[i] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m, output[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m), fill\u001b[38;5;241m=\u001b[39mcolors[i \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Show the image\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m image\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\mlenv312\\Lib\\site-packages\\PIL\\Image.py:2514\u001b[0m, in \u001b[0;36mImage.show\u001b[1;34m(self, title)\u001b[0m\n\u001b[0;32m   2494\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, title: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2495\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2496\u001b[0m \u001b[38;5;124;03m    Displays this image. This method is mainly intended for debugging purposes.\u001b[39;00m\n\u001b[0;32m   2497\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2511\u001b[0m \u001b[38;5;124;03m    :param title: Optional title to use for the image window, where possible.\u001b[39;00m\n\u001b[0;32m   2512\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2514\u001b[0m     _show(\u001b[38;5;28mself\u001b[39m, title\u001b[38;5;241m=\u001b[39mtitle)\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\mlenv312\\Lib\\site-packages\\PIL\\Image.py:3571\u001b[0m, in \u001b[0;36m_show\u001b[1;34m(image, **options)\u001b[0m\n\u001b[0;32m   3568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_show\u001b[39m(image, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3569\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageShow\n\u001b[1;32m-> 3571\u001b[0m     ImageShow\u001b[38;5;241m.\u001b[39mshow(image, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\mlenv312\\Lib\\site-packages\\PIL\\ImageShow.py:64\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(image, title, **options)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03mDisplay a given image.\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m:returns: ``True`` if a suitable viewer was found, ``False`` otherwise.\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m viewer \u001b[38;5;129;01min\u001b[39;00m _viewers:\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m viewer\u001b[38;5;241m.\u001b[39mshow(image, title\u001b[38;5;241m=\u001b[39mtitle, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions):\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\mlenv312\\Lib\\site-packages\\PIL\\ImageShow.py:88\u001b[0m, in \u001b[0;36mViewer.show\u001b[1;34m(self, image, **options)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m!=\u001b[39m base:\n\u001b[0;32m     86\u001b[0m         image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mconvert(base)\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_image(image, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\mlenv312\\Lib\\site-packages\\PIL\\ImageShow.py:115\u001b[0m, in \u001b[0;36mViewer.show_image\u001b[1;34m(self, image, **options)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: Image\u001b[38;5;241m.\u001b[39mImage, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m    114\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Display the given image.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_file(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_image(image), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\mlenv312\\Lib\\site-packages\\PIL\\ImageShow.py:145\u001b[0m, in \u001b[0;36mWindowsViewer.show_file\u001b[1;34m(self, path, **options)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow_file\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m    142\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03m    Display given file.\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m     subprocess\u001b[38;5;241m.\u001b[39mPopen(\n\u001b[0;32m    146\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_command(path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions),\n\u001b[0;32m    147\u001b[0m         shell\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    148\u001b[0m         creationflags\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(subprocess, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCREATE_NO_WINDOW\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    149\u001b[0m     )  \u001b[38;5;66;03m# nosec\u001b[39;00m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\mlenv312\\Lib\\subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[0;32m   1022\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[0;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0;32m   1027\u001b[0m                         pass_fds, cwd, env,\n\u001b[0;32m   1028\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[0;32m   1029\u001b[0m                         p2cread, p2cwrite,\n\u001b[0;32m   1030\u001b[0m                         c2pread, c2pwrite,\n\u001b[0;32m   1031\u001b[0m                         errread, errwrite,\n\u001b[0;32m   1032\u001b[0m                         restore_signals,\n\u001b[0;32m   1033\u001b[0m                         gid, gids, uid, umask,\n\u001b[0;32m   1034\u001b[0m                         start_new_session, process_group)\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\mlenv312\\Lib\\subprocess.py:1538\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session, unused_process_group)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# Start the process\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1538\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m _winapi\u001b[38;5;241m.\u001b[39mCreateProcess(executable, args,\n\u001b[0;32m   1539\u001b[0m                              \u001b[38;5;66;03m# no special security\u001b[39;00m\n\u001b[0;32m   1540\u001b[0m                              \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1541\u001b[0m                              \u001b[38;5;28mint\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m close_fds),\n\u001b[0;32m   1542\u001b[0m                              creationflags,\n\u001b[0;32m   1543\u001b[0m                              env,\n\u001b[0;32m   1544\u001b[0m                              cwd,\n\u001b[0;32m   1545\u001b[0m                              startupinfo)\n\u001b[0;32m   1546\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1547\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1548\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_pipe_fds(p2cread, p2cwrite,\n\u001b[0;32m   1554\u001b[0m                          c2pread, c2pwrite,\n\u001b[0;32m   1555\u001b[0m                          errread, errwrite)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "folder_path = \"test\"  # Change this to the path of your image folder\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "    ]\n",
    ")\n",
    "model = MediumUNet().to(device)\n",
    "model.load_state_dict(torch.load(\"model_8.pth\"))\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # Adjust file extensions as needed\n",
    "            print(filename)\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            image = Image.open(image_path)\n",
    "            if image.mode == 'RGB':\n",
    "                image = image.convert('L')\n",
    "            image = image.convert('RGB')\n",
    "            image_tensor = transform(image)\n",
    "            original_image_size = image.size[::-1]\n",
    "            \n",
    "            # Get the model output\n",
    "            output = model(image_tensor.to(device).unsqueeze(0))\n",
    "            back = transforms.Compose([transforms.ToPILImage()])\n",
    "            output = output.squeeze().cpu().numpy()\n",
    "            image = back(image_tensor)\n",
    "            draw = PIL.ImageDraw.Draw(image)\n",
    "            colors = [\n",
    "                'red', 'green', 'blue', 'yellow', 'purple', 'orange', 'cyan', 'magenta', 'lime', 'pink',\n",
    "                'teal', 'lavender', 'brown', 'beige'\n",
    "            ]\n",
    "            for i in range(0, len(output), 2):\n",
    "                draw.ellipse((output[i] - 2, output[i + 1] - 2, output[i] + 2, output[i + 1] + 2), fill=colors[i // 2])\n",
    "            # Show the image\n",
    "            image.show()\n",
    "            print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
